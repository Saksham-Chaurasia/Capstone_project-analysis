{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b365f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark==3.2.4\n",
      "  Using cached pyspark-3.2.4.tar.gz (281.5 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.5\n",
      "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.4-py2.py3-none-any.whl size=282040957 sha256=1ab626d1eafd90ae38117370ae3b60f5e95ce0d15c97a9aac04df000930cf693\n",
      "  Stored in directory: c:\\users\\saksh\\appdata\\local\\pip\\cache\\wheels\\bf\\f6\\18\\acaa11d057c23e749eea1773c9516d58fbc8ddcdd1492ad3fd\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.7\n",
      "    Can't uninstall 'py4j'. No files were found to uninstall.\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.4.0\n",
      "    Can't uninstall 'pyspark'. No files were found to uninstall.\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for pyspark: [Errno 2] No such file or directory: 'c:\\\\users\\\\saksh\\\\appdata\\\\roaming\\\\python\\\\python310\\\\site-packages\\\\pyspark-3.4.0.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for py4j: [Errno 2] No such file or directory: 'c:\\\\users\\\\saksh\\\\appdata\\\\roaming\\\\python\\\\python310\\\\site-packages\\\\py4j-0.10.9.7.dist-info\\\\METADATA'\n",
      "    WARNING: No metadata found in c:\\users\\saksh\\appdata\\roaming\\python\\python310\\site-packages\n",
      "    WARNING: No metadata found in c:\\users\\saksh\\appdata\\roaming\\python\\python310\\site-packages\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark==3.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffd6bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b106677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n",
      "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "|age|         job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n",
      "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "| 58|  management| married| tertiary|     no|   2143|    yes|  no|unknown|  5|  may|     261|       1|   -1|       0| unknown| no|\n",
      "| 44|  technician|  single|secondary|     no|     29|    yes|  no|unknown|  5|  may|     151|       1|   -1|       0| unknown| no|\n",
      "| 33|entrepreneur| married|secondary|     no|      2|    yes| yes|unknown|  5|  may|      76|       1|   -1|       0| unknown| no|\n",
      "| 47| blue-collar| married|  unknown|     no|   1506|    yes|  no|unknown|  5|  may|      92|       1|   -1|       0| unknown| no|\n",
      "| 33|     unknown|  single|  unknown|     no|      1|     no|  no|unknown|  5|  may|     198|       1|   -1|       0| unknown| no|\n",
      "| 35|  management| married| tertiary|     no|    231|    yes|  no|unknown|  5|  may|     139|       1|   -1|       0| unknown| no|\n",
      "| 28|  management|  single| tertiary|     no|    447|    yes| yes|unknown|  5|  may|     217|       1|   -1|       0| unknown| no|\n",
      "| 42|entrepreneur|divorced| tertiary|    yes|      2|    yes|  no|unknown|  5|  may|     380|       1|   -1|       0| unknown| no|\n",
      "| 58|     retired| married|  primary|     no|    121|    yes|  no|unknown|  5|  may|      50|       1|   -1|       0| unknown| no|\n",
      "| 43|  technician|  single|secondary|     no|    593|    yes|  no|unknown|  5|  may|      55|       1|   -1|       0| unknown| no|\n",
      "| 41|      admin.|divorced|secondary|     no|    270|    yes|  no|unknown|  5|  may|     222|       1|   -1|       0| unknown| no|\n",
      "| 29|      admin.|  single|secondary|     no|    390|    yes|  no|unknown|  5|  may|     137|       1|   -1|       0| unknown| no|\n",
      "| 53|  technician| married|secondary|     no|      6|    yes|  no|unknown|  5|  may|     517|       1|   -1|       0| unknown| no|\n",
      "| 58|  technician| married|  unknown|     no|     71|    yes|  no|unknown|  5|  may|      71|       1|   -1|       0| unknown| no|\n",
      "| 57|    services| married|secondary|     no|    162|    yes|  no|unknown|  5|  may|     174|       1|   -1|       0| unknown| no|\n",
      "| 51|     retired| married|  primary|     no|    229|    yes|  no|unknown|  5|  may|     353|       1|   -1|       0| unknown| no|\n",
      "| 45|      admin.|  single|  unknown|     no|     13|    yes|  no|unknown|  5|  may|      98|       1|   -1|       0| unknown| no|\n",
      "| 57| blue-collar| married|  primary|     no|     52|    yes|  no|unknown|  5|  may|      38|       1|   -1|       0| unknown| no|\n",
      "| 60|     retired| married|  primary|     no|     60|    yes|  no|unknown|  5|  may|     219|       1|   -1|       0| unknown| no|\n",
      "| 33|    services| married|secondary|     no|      0|    yes|  no|unknown|  5|  may|      54|       1|   -1|       0| unknown| no|\n",
      "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ ==\"__main__\":\n",
    "\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"maarketdata\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    schema = \"age INT, job STRING , marital STRING , education STRING, default STRING, balance INT,\\\n",
    "     housing STRING, loan STRING, contact STRING, day INT, month STRING, \\\n",
    "    duration INT, campaign INT, pdays INT, previous INT, poutcome STRING, y STRING \"\n",
    "\n",
    "    data = spark.read.csv(\"D:/Projectssssss/Capstone Project/marketanalysisdata.csv\",\n",
    "                          schema=schema,sep=\",\",header=True)\n",
    "#     selected_data = data.select(\"age\",\"job\",\"marital\",\"education\")\n",
    "\n",
    "    data.printSchema()\n",
    "    data.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85c99a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "rdd=spark.sparkContext.parallelize(dataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdb17971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58cace21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n",
      "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "|age|         job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n",
      "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "| 41|  management| married| tertiary|     no|     65|    yes|  no|unknown|  6|  may|     524|       2|   -1|       0| unknown| no|\n",
      "| 41|  technician| married|  unknown|     no|    254|    yes|  no|unknown|  8|  may|     690|       1|   -1|       0| unknown| no|\n",
      "| 41|  technician| married|secondary|     no|   -366|    yes| yes|unknown|  6|  may|      29|       3|   -1|       0| unknown| no|\n",
      "| 41|  technician| married|secondary|     no|   1270|    yes|  no|unknown|  5|  may|    1389|       1|   -1|       0| unknown|yes|\n",
      "| 41|    services| married|  unknown|     no|      4|     no|  no|unknown|  6|  may|     284|       2|   -1|       0| unknown| no|\n",
      "| 41|    services| married|secondary|     no|      0|    yes|  no|unknown|  5|  may|     114|       2|   -1|       0| unknown| no|\n",
      "| 41|entrepreneur| married|  unknown|     no|     89|    yes|  no|unknown|  6|  may|     333|       2|   -1|       0| unknown| no|\n",
      "| 41|  management| married| tertiary|     no|    517|    yes|  no|unknown|  5|  may|     251|       1|   -1|       0| unknown| no|\n",
      "| 41| blue-collar| married|secondary|     no|    140|    yes|  no|unknown|  6|  may|     311|       3|   -1|       0| unknown| no|\n",
      "| 41| blue-collar| married|  primary|     no|    183|    yes| yes|unknown|  5|  may|     110|       2|   -1|       0| unknown| no|\n",
      "| 41|      admin.| married|secondary|     no|      0|    yes|  no|unknown|  6|  may|     160|       3|   -1|       0| unknown| no|\n",
      "| 41| blue-collar| married|secondary|     no|    290|    yes|  no|unknown|  5|  may|     240|       2|   -1|       0| unknown| no|\n",
      "| 41|      admin.|divorced|secondary|     no|    322|    yes|  no|unknown|  6|  may|      87|       4|   -1|       0| unknown| no|\n",
      "| 41|  technician| married|secondary|     no|    871|    yes|  no|unknown|  6|  may|     145|       1|   -1|       0| unknown| no|\n",
      "| 41| blue-collar| married|  primary|     no|    512|    yes|  no|unknown|  6|  may|     233|       2|   -1|       0| unknown| no|\n",
      "| 41| blue-collar| married|secondary|     no|    946|    yes|  no|unknown|  6|  may|     325|       1|   -1|       0| unknown| no|\n",
      "| 41|  technician| married|secondary|     no|    169|    yes|  no|unknown|  7|  may|     296|       2|   -1|       0| unknown| no|\n",
      "| 41| blue-collar|  single|  primary|    yes|   -137|    yes| yes|unknown|  6|  may|     189|       1|   -1|       0| unknown| no|\n",
      "| 41|  management|divorced|secondary|     no|    276|    yes|  no|unknown|  7|  may|      60|       1|   -1|       0| unknown| no|\n",
      "| 41| blue-collar| married|  unknown|     no|    225|    yes|  no|unknown|  7|  may|     690|       2|   -1|       0| unknown| no|\n",
      "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.createOrReplaceTempView(\"sample\")\n",
    "df2=spark.sql(\"select * from sample where age > 40 order by age \")\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e0a642c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  y|  cnt|\n",
      "+---+-----+\n",
      "| no|39922|\n",
      "|yes| 5289|\n",
      "+---+-----+\n",
      "\n",
      "+---------+\n",
      "|total_cnt|\n",
      "+---------+\n",
      "|    45211|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# giving marketing success rate\n",
    "\n",
    "df3 = spark.sql(\"select y,count(*) as cnt from sample group by y\")\n",
    "df4 = spark.sql(\"select count(*) as total_cnt from sample\")\n",
    "df3.show()\n",
    "df4.show()\n",
    "\n",
    "df4.createOrReplaceTempView(\"total\")\n",
    "df3.createOrReplaceTempView(\"market\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "06cfd61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------------+------------------+\n",
      "|  y|  cnt|(total_cnt - cnt)|           percent|\n",
      "+---+-----+-----------------+------------------+\n",
      "| no|39922|             5289| 88.30151954170445|\n",
      "|yes| 5289|            39922|11.698480458295547|\n",
      "+---+-----+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# market success rate and failure \n",
    "df5=spark.sql(\"select m.y,m.cnt , t.total_cnt-m.cnt, m.cnt/t.total_cnt * 100 as percent from market m left join total t \")\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8de9f39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\saksh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\saksh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saksh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\saksh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\saksh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\saksh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "029cdade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+\n",
      "|Max_age|Min_age|          mean_age|\n",
      "+-------+-------+------------------+\n",
      "|     95|     18|41.670069956513515|\n",
      "+-------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6 = spark.sql(\"select max(age) as Max_age, min(age) as Min_age, Avg(age) as mean_age from sample where y='yes'\")\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "21f89e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|      mean_balance|median_balance|\n",
      "+------------------+--------------+\n",
      "|1362.2720576850766|           448|\n",
      "+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7 = spark.sql(\"select avg(balance) as mean_balance,percentile_approx(balance,0.5) as median_balance from sample \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c1df9019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|age|cnt|\n",
      "+---+---+\n",
      "| 95|  1|\n",
      "| 93|  2|\n",
      "| 92|  2|\n",
      "| 90|  2|\n",
      "| 87|  3|\n",
      "| 85|  4|\n",
      "| 86|  4|\n",
      "| 84|  5|\n",
      "| 81|  6|\n",
      "| 83|  6|\n",
      "| 18|  7|\n",
      "| 82|  8|\n",
      "| 79| 10|\n",
      "| 19| 11|\n",
      "| 80| 12|\n",
      "| 74| 13|\n",
      "| 78| 14|\n",
      "| 20| 15|\n",
      "| 75| 15|\n",
      "| 76| 16|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8 = spark.sql(\"select age,count(*) as cnt from sample where y ='yes' group by age order by cnt\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "501c1643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "| marital| cnt|\n",
      "+--------+----+\n",
      "|divorced| 622|\n",
      "|  single|1912|\n",
      "| married|2755|\n",
      "+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df9 = spark.sql(\"select marital,count(*)as cnt from sample where y = 'yes' group by marital order by cnt\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f3545f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age| marital|cnt|deposit|\n",
      "+---+--------+---+-------+\n",
      "| 18|  single| 12|      7|\n",
      "| 19|  single| 35|     11|\n",
      "| 20| married|  3|      1|\n",
      "| 20|  single| 47|     14|\n",
      "| 21| married|  5|      1|\n",
      "| 21|  single| 74|     21|\n",
      "| 22| married|  9|      0|\n",
      "| 22|  single|120|     40|\n",
      "| 23| married| 27|      2|\n",
      "| 23|  single|175|     42|\n",
      "| 24|divorced|  1|      0|\n",
      "| 24| married| 53|     10|\n",
      "| 24|  single|248|     58|\n",
      "| 25|divorced|  6|      0|\n",
      "| 25| married| 98|     14|\n",
      "| 25|  single|423|     99|\n",
      "| 26|divorced| 20|      0|\n",
      "| 26| married|170|     13|\n",
      "| 26|  single|615|    121|\n",
      "| 27|divorced| 18|      2|\n",
      "+---+--------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df10 = spark.sql(\"select age, marital,count(*) as cnt,sum(case when y = 'yes' then 1 else 0 end) as deposit from sample group by age, marital order by age, marital\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5195a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+-------------+\n",
      "|age|         job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|  y|age_featuring|\n",
      "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+-------------+\n",
      "| 58|  management| married| tertiary|     no|   2143|    yes|  no|unknown|  5|  may|     261|       1|   -1|       0| unknown| no|        50-60|\n",
      "| 44|  technician|  single|secondary|     no|     29|    yes|  no|unknown|  5|  may|     151|       1|   -1|       0| unknown| no|        40-50|\n",
      "| 33|entrepreneur| married|secondary|     no|      2|    yes| yes|unknown|  5|  may|      76|       1|   -1|       0| unknown| no|        30-40|\n",
      "| 47| blue-collar| married|  unknown|     no|   1506|    yes|  no|unknown|  5|  may|      92|       1|   -1|       0| unknown| no|        40-50|\n",
      "| 33|     unknown|  single|  unknown|     no|      1|     no|  no|unknown|  5|  may|     198|       1|   -1|       0| unknown| no|        30-40|\n",
      "| 35|  management| married| tertiary|     no|    231|    yes|  no|unknown|  5|  may|     139|       1|   -1|       0| unknown| no|        30-40|\n",
      "| 28|  management|  single| tertiary|     no|    447|    yes| yes|unknown|  5|  may|     217|       1|   -1|       0| unknown| no|        20-30|\n",
      "| 42|entrepreneur|divorced| tertiary|    yes|      2|    yes|  no|unknown|  5|  may|     380|       1|   -1|       0| unknown| no|        40-50|\n",
      "| 58|     retired| married|  primary|     no|    121|    yes|  no|unknown|  5|  may|      50|       1|   -1|       0| unknown| no|        50-60|\n",
      "| 43|  technician|  single|secondary|     no|    593|    yes|  no|unknown|  5|  may|      55|       1|   -1|       0| unknown| no|        40-50|\n",
      "| 41|      admin.|divorced|secondary|     no|    270|    yes|  no|unknown|  5|  may|     222|       1|   -1|       0| unknown| no|        40-50|\n",
      "| 29|      admin.|  single|secondary|     no|    390|    yes|  no|unknown|  5|  may|     137|       1|   -1|       0| unknown| no|        20-30|\n",
      "| 53|  technician| married|secondary|     no|      6|    yes|  no|unknown|  5|  may|     517|       1|   -1|       0| unknown| no|        50-60|\n",
      "| 58|  technician| married|  unknown|     no|     71|    yes|  no|unknown|  5|  may|      71|       1|   -1|       0| unknown| no|        50-60|\n",
      "| 57|    services| married|secondary|     no|    162|    yes|  no|unknown|  5|  may|     174|       1|   -1|       0| unknown| no|        50-60|\n",
      "| 51|     retired| married|  primary|     no|    229|    yes|  no|unknown|  5|  may|     353|       1|   -1|       0| unknown| no|        50-60|\n",
      "| 45|      admin.|  single|  unknown|     no|     13|    yes|  no|unknown|  5|  may|      98|       1|   -1|       0| unknown| no|        40-50|\n",
      "| 57| blue-collar| married|  primary|     no|     52|    yes|  no|unknown|  5|  may|      38|       1|   -1|       0| unknown| no|        50-60|\n",
      "| 60|     retired| married|  primary|     no|     60|    yes|  no|unknown|  5|  may|     219|       1|   -1|       0| unknown| no|        50-60|\n",
      "| 33|    services| married|secondary|     no|      0|    yes|  no|unknown|  5|  may|      54|       1|   -1|       0| unknown| no|        30-40|\n",
      "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df11 = spark.sql(\"select * ,case when age<=20 then '20-' \\\n",
    "                  when age >20 and age<=30 then '20-30' \\\n",
    "                  when age >30 and age<=40 then '30-40' \\\n",
    "                  when age >40 and age<=50 then '40-50' \\\n",
    "                  when age >50 and age<=60 then '50-60' \\\n",
    "                  when age >60 and age<=70 then '60-70' \\\n",
    "                     else '70+' end as age_featuring from sample\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "35ef0894",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'createOrReplaceTempView'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf11\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateOrReplaceTempView\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maged\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m df12 \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect age_featuring from aged group by age_featuring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'createOrReplaceTempView'"
     ]
    }
   ],
   "source": [
    "df11.createOrReplaceTempView(\"aged\")\n",
    "df12 = spark.sql(\"select age_featuring from aged group by age_featuring\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16be1baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a8b5f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema2  = \"id INT, gender STRING, marks INT\"\n",
    "dat = spark.read.csv(\"D:/Projectssssss/Capstone Project/ct.csv\",\n",
    "                          schema=schema2,sep=\",\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5fc0f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| id|gender|marks|\n",
      "+---+------+-----+\n",
      "|  2|     m|   50|\n",
      "|  3|     m|   70|\n",
      "|  4|     m|   80|\n",
      "|  5|     f|   20|\n",
      "|  6|     f|   30|\n",
      "|  7|     f|   30|\n",
      "|  8|     f|   40|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dat.createOrReplaceTempView(\"window\")\n",
    "dat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1e31a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+\n",
      "| id|gender|marks|row_num|\n",
      "+---+------+-----+-------+\n",
      "|  5|     f|   20|      1|\n",
      "|  6|     f|   30|      2|\n",
      "|  7|     f|   30|      3|\n",
      "|  8|     f|   40|      4|\n",
      "|  2|     m|   50|      1|\n",
      "|  3|     m|   70|      2|\n",
      "|  4|     m|   80|      3|\n",
      "+---+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dats = spark.sql(\"select *, row_number() over (partition by gender order by marks)as row_num from window \")\n",
    "dats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da187ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+----+\n",
      "| id|gender|marks|rank|\n",
      "+---+------+-----+----+\n",
      "|  5|     f|   20|   1|\n",
      "|  6|     f|   30|   2|\n",
      "|  7|     f|   30|   2|\n",
      "|  8|     f|   40|   4|\n",
      "|  2|     m|   50|   1|\n",
      "|  3|     m|   70|   2|\n",
      "|  4|     m|   80|   3|\n",
      "+---+------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dats2 = spark.sql(\"select *, rank() over (partition by gender order by marks)as rank from window \")\n",
    "dats2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fcc6caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+----------+\n",
      "| id|gender|marks|dense_rank|\n",
      "+---+------+-----+----------+\n",
      "|  5|     f|   20|         1|\n",
      "|  6|     f|   30|         2|\n",
      "|  7|     f|   30|         2|\n",
      "|  8|     f|   40|         3|\n",
      "|  2|     m|   50|         1|\n",
      "|  3|     m|   70|         2|\n",
      "|  4|     m|   80|         3|\n",
      "+---+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dats3 = spark.sql(\"select *, dense_rank() over (partition by gender order by marks)as dense_rank from window \")\n",
    "dats3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f323e19",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "expression 'sub.id' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;\nAggregate [gender#361], [id#360, gender#361, max(dense_rank#443) AS rank#501]\n+- SubqueryAlias sub\n   +- View (`sub`, [id#360,gender#361,marks#362,dense_rank#443])\n      +- Project [id#360, gender#361, marks#362, dense_rank#443]\n         +- Project [id#360, gender#361, marks#362, dense_rank#443, dense_rank#443]\n            +- Window [dense_rank(marks#362) windowspecdefinition(gender#361, marks#362 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS dense_rank#443], [gender#361], [marks#362 ASC NULLS FIRST]\n               +- Project [id#360, gender#361, marks#362]\n                  +- SubqueryAlias window\n                     +- View (`window`, [id#360,gender#361,marks#362])\n                        +- Relation [id#360,gender#361,marks#362] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dats3\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m dats4 \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect id, gender,max(dense_rank) as rank from sub group by gender\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m dats4\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\sql\\session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \n\u001b[0;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: expression 'sub.id' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;\nAggregate [gender#361], [id#360, gender#361, max(dense_rank#443) AS rank#501]\n+- SubqueryAlias sub\n   +- View (`sub`, [id#360,gender#361,marks#362,dense_rank#443])\n      +- Project [id#360, gender#361, marks#362, dense_rank#443]\n         +- Project [id#360, gender#361, marks#362, dense_rank#443, dense_rank#443]\n            +- Window [dense_rank(marks#362) windowspecdefinition(gender#361, marks#362 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS dense_rank#443], [gender#361], [marks#362 ASC NULLS FIRST]\n               +- Project [id#360, gender#361, marks#362]\n                  +- SubqueryAlias window\n                     +- View (`window`, [id#360,gender#361,marks#362])\n                        +- Relation [id#360,gender#361,marks#362] csv\n"
     ]
    }
   ],
   "source": [
    "# dats3.createOrReplaceTempView(\"sub\")\n",
    "# dats4 = spark.sql(\"select gender,max(dense_rank) as rank from sub group by gender\")\n",
    "# dats4.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30f845b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8cf4e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- campaign: string (nullable = true)\n",
      " |-- pdays: string (nullable = true)\n",
      " |-- previous: string (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newd = spark.read.csv(\"D:/Projectssssss/Capstone Project/marketanalysisdata.csv\",header=True)\n",
    "newd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0ef70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "newd2 =newd.withColumnRenamed(\"age\",\"Age\")\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0f278ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- campaign: string (nullable = true)\n",
      " |-- pdays: string (nullable = true)\n",
      " |-- previous: string (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newd2.printSchema()\n",
    "newd3 = newd2.withColumn(\"Age\",col(\"Age\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ecaf7895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5400"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newd2.filter((newd2.marital=='married') &(newd2.job=='management')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78624cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
